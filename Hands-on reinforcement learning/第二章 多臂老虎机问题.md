# 第二章 多臂老虎机问题

## 一、问题介绍

1. 问题定义
   + 多臂老虎机(MAB)问题：拥有 $K$ 根拉杆的老虎机，每根拉杆对应一个奖励概率分布
   + 目标：在 $T$ 次操作中最大化累积奖励

2. 形式化描述
   + 动作集合  $\mathcal{A}$ ：表示拉动的拉杆
   + 奖励概率分布  $\mathcal{R}$ ：每根拉杆对应一个奖励概率分布
   + 目标：最大化累积奖励  $\max \sum_{t=1}^{T} r_t$ 
3. 累计懊悔
   + 期望奖励  $Q(a) = E_{r \sim \mathcal{R}(·|a)}[r]$ ：每个动作的期望奖励
   + 最优期望奖励  $Q^* = \max_{a \in \mathcal{A}} Q(a)$ ：所有动作中的最大期望奖励
   + 懊悔(regret)  $R(a) = Q^* - Q(a)$ ：当前动作与最优动作的期望奖励差
   + 累积懊悔  $\sigma_R = \sum_{t=1}^{T} R(a_t)$ ：$T$ 次操作后的总懊悔
   + 目标：最大化累积奖励 = 最小化累积懊悔

4. 估计期望奖励

   + 算法流程
     $$
     \begin{equation}
     \begin{aligned}
     & \text{对于 } \forall a \in \mathcal{A}, \text{ 初始化计数器 } N(a) = 0 \text{ 和期望奖励估值 } \hat{Q}(a) = 0 \\
     & \text{for } t = 1 \rightarrow T \text{ do} \\
     & \quad \text{选取某根拉杆，该动作记为 } a_t \\
     & \quad \text{得到奖励 } r_t \\
     & \quad \text{更新计数器：} N(a_t) = N(a_t) + 1 \\
     & \quad \text{更新期望奖励估值：} \hat{Q}(a_t) = \hat{Q}(a_t) + \frac{1}{N(a_t)} [r_t - \hat{Q}(a_t)] \\
     & \text{end for}
     \end{aligned}
     \end{equation}
     $$

   + 增量式更新
     $$
     \begin{equation}
     \begin{aligned}
     Q_k &= \frac{1}{k} \sum_{i=1}^{k} r_i \\
     &= \frac{1}{k} \left( r_k + \sum_{i=1}^{k-1} r_i \right) \\
     &= \frac{1}{k} (r_k + (k-1)Q_{k-1}) \\
     &= \frac{1}{k} (r_k + kQ_{k-1} - Q_{k-1}) \\
     &= Q_{k-1} + \frac{1}{k} [r_k - Q_{k-1}]
     \end{aligned}
     \end{equation}
     $$

     + 优点：时间复杂度和空间复杂度均为 $O(1)$，传统方法时间复杂度和空间复杂度均为 $O(n)$

   + 代码实现

     + 多臂老虎机

       ```python
       # 导入需要使用的库，其中 numpy 是支持数组和矩阵运算的科学计算库，而 matplotlib 是绘图库
       import numpy as np
       import matplotlib.pyplot as plt
       
       class BernoulliBandit:
           """伯努利多臂老虎机，输入 K 表示拉杆个数"""
           def __init__(self, K):
               self.probs = np.random.uniform(size=K)  # 随机生成 K 个 0~1 的数，作为拉动每根拉杆的获奖概率
               self.best_idx = np.argmax(self.probs)  # 获奖概率最大的拉杆
               self.best_prob = self.probs[self.best_idx]  # 最大的获奖概率
               self.K = K
       
           def step(self, k):
               # 当玩家选择了 k 号拉杆后，根据拉动该老虎机的 k 号拉杆获得奖励的概率返回 1（获奖）或 0（未获奖）
               if np.random.rand() < self.probs[k]:
                   return 1
               else:
                   return 0
       
       np.random.seed(1)  # 设定随机种子，使实验具有可重复性
       
       K = 10
       bandit_10_arm = BernoulliBandit(K)
       print("随机生成了一个%d臂伯努利老虎机" % K)
       print("获奖概率最大的拉杆为%d号，其获奖概率为%.4f" % (bandit_10_arm.best_idx, bandit_10_arm.best_prob))
       ```

     + 多臂老虎机求解

       ```python
       class Solver:
           """多臂老虎机算法基本框架"""
           def __init__(self, bandit):
               self.bandit = bandit
               self.counts = np.zeros(self.bandit.K)  # 每根拉杆的尝试次数
               self.regret = 0  # 当前步的累积后悔
               self.actions = []  # 维护一个列表，记录每一步的动作
               self.regrets = []  # 维护一个列表，记录每一步的累积后悔
       
           def update_regret(self, k):
               # 计算累积后悔并保存，k为本次动作选择的拉杆的编号
               self.regret += self.bandit.best_prob - self.bandit.probs[k]
               self.regrets.append(self.regret)
       
           def run_one_step(self):
               # 返回当前动作选择哪一根拉杆，由每个具体的策略实现
               raise NotImplementedError
       
           def run(self, num_steps):
               # 运行一定次数，num_steps为总运行次数
               for _ in range(num_steps):
                   k = self.run_one_step()
                   self.counts[k] += 1
                   self.actions.append(k)
                   self.update_regret(k)
       ```

## 二、探索与利用的平衡

+ 探索(exploration)：尝试更多可能的拉杆，以获取更多信息
+ 利用(exploitation)：选择已知期望奖励最大的拉杆，当前最优拉杆不一定是全局最优

## 三、$\epsilon$ -贪婪算法

1. $\epsilon$ -贪婪算法

   + 完全贪婪算法：总是选择当前期望奖励最大的拉杆。只有利用，没有探索

   + $\epsilon$ -贪婪算法：在完全贪婪算法的基础上添加了随机选择（探索）的概率 $\epsilon$
     + 利用：以概率 $1 - \epsilon$ 选择期望奖励最大的拉杆
     + 探索：以概率 $\epsilon$ 随机选择一根拉杆
   + 随着探索次数增加，奖励估计越来越准确，探索概率 $\epsilon$ 可以随时间衰减。因为有限步数的完全贪婪算法是局部信息的贪婪算法，所有 $\epsilon$ 不会在有限步数内衰减至0

2. $\epsilon$ -贪婪算法实现

   + $\epsilon$ -贪婪算法的累积懊悔随时间线性增长。随着 $\epsilon$ 增大，累积懊悔增长的速率也增大

   ```python
   class EpsilonGreedy(Solver):
       """ epsilon 贪婪算法, 继承 Solver 类 """
       def __init__(self, bandit, epsilon=0.01, init_prob=1.0):
           super(EpsilonGreedy, self).__init__(bandit)
           self.epsilon = epsilon
           # 初始化拉动所有拉杆的期望奖励估值
           self.estimates = np.array([init_prob] * self.bandit.K)
   
       def run_one_step(self):
           if np.random.random() < self.epsilon:
               k = np.random.randint(0, self.bandit.K)  # 随机选择一根拉杆
           else:
               k = np.argmax(self.estimates)  # 选择期望奖励估值最大的拉杆
           r = self.bandit.step(k)  # 得到本次动作的奖励
           self.estimates[k] += 1. / (self.counts[k] + 1) * (r - self.estimates[k])
           return k
   ```

3. 衰减 $\epsilon$ -贪婪算法

   + $\epsilon$ 值随时间衰减，采用反比例衰减形式：$ \epsilon_t = \frac{1}{t} $
   + 随着时间推移，探索概率逐渐降低，累积懊悔与时间步的关系变为次线性

   ```python
   class DecayingEpsilonGreedy(Solver):
       """epsilon值随时间衰减的epsilon-贪婪算法，继承Solver类"""
       def __init__(self, bandit, init_prob=1.0):
           super(DecayingEpsilonGreedy, self).__init__(bandit)
           self.estimates = np.array([init_prob] * self.bandit.K)
           self.total_count = 0
   
       def run_one_step(self):
           self.total_count += 1
           if np.random.random() < 1 / self.total_count:  # epsilon值随时间衰减
               k = np.random.randint(0, self.bandit.K)
           else:
               k = np.argmax(self.estimates)
           r = self.bandit.step(k)
           self.estimates[k] += 1. / (self.counts[k] + 1) * (r - self.estimates[k])
           return k
   ```

## 四、上置信界算法

1. 上置信界(UCB)算法：一种基于不确定性的策略算法，通过估计每根拉杆的期望奖励的上界来选择拉杆，从而在探索和利用之间取得平衡

2. 核心思想

   + 不确定性度量：UCB算法使用不确定性度量 $ U(a) $ 来评估每根拉杆的探索价值。不确定性越大，探索的价值越高
   + 霍夫丁不等式：利用霍夫丁不等式估计不确定性，公式为 $ \hat{U}(a) = \sqrt{\frac{\log t}{2(N(a)+1)}} $，其中 $ t $ 是总的尝试次数，$ N(a) $ 是拉杆 $ a $ 被尝试的次数
   + 选择策略：选择期望奖励上界最大的拉杆，即 $ a = \arg\max_{a \in A} [\hat{Q}(a) + c \cdot \hat{U}(a)] $，其中 $ c $ 是控制不确定性比重的系数

3. 上置信界算法实现

   ```python
   class UCB(Solver):
       """ UCB算法，继承Solver类 """
       def __init__(self, bandit, coef, init_prob=1.0):
           super(UCB, self).__init__(bandit)
           self.total_count = 0
           self.estimates = np.array([init_prob] * self.bandit.K)
           self.coef = coef
   
       def run_one_step(self):
           self.total_count += 1
           ucb = self.estimates + self.coef * np.sqrt(np.log(self.total_count) / (2 * (self.counts + 1)))  # 计算上置信界
           k = np.argmax(ucb)  # 选出上置信界最大的拉杆
           r = self.bandit.step(k)
           self.estimates[k] += 1. / (self.counts[k] + 1) * (r - self.estimates[k])
           return k
   
   np.random.seed(1)
   coef = 1  # 控制不确定性比重的系数
   UCB_solver = UCB(bandit_10_arm, coef)
   UCB_solver.run(5000)
   print('上置信界算法的累积懊悔为:', UCB_solver.regret)
   plot_results([UCB_solver], ["UCB"])
   ```

## 五、汤普森采样算法

1. 汤普森采样算法
   + 该算法假设每根拉杆的奖励服从一个特定的概率分布，并根据期望奖励进行选择
   + 由于直接计算所有拉杆的期望奖励代价较高，汤普森采样通过采样方式进行选择
2. 核心思想
   + 根据当前每个动作的奖励概率分布进行一轮采样，得到各拉杆的奖励样本
   + 选择样本中奖励最大的动作
   + 该算法是一种计算所有拉杆最高奖励概率的蒙特卡洛采样方法

3. 概率分布建模

   + 通常使用 Beta 分布对每个动作的奖励概率分布进行建模
   + 若某拉杆被选择 $ k $ 次，其中 $ m_1 $ 次奖励为 1，$ m_2 $ 次奖励为 0，则该拉杆的奖励服从参数为 $ (m_1+1, m_2+1) $ 的 Beta 分布

4. 汤普森采样算法实现

   

## 六、总结

+ $\epsilon$ -贪婪算法的累积懊悔随时间线性增长

  $\epsilon$ -衰减贪婪算法、上置信界算法、汤普森采样算法的累积懊悔随时间次线性增长

+ 多臂老虎机问题是无状态的强化学习(stateless reinforcement learning)：多臂老虎机问题与强化学习的区别在于其与环境的交互不会改变环境，即每次交互的结果和以往的动作无关