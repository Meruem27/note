# 第一章 初探强化学习

## 一、强化学习基本概念

1. 强化学习(reinforcement learning)：机器通过与环境交互来实现目标的计算方法
2. 智能体(agent)：做决策的机器
   + 感知：感知环境的状态
   + 决策：根据当前状态计算出达到目标需要采取的动作的过程
   + 奖励：环境根据状态和智能体采取的动作，产生一个标量信号作为奖励反馈

## 二、强化学习的环境

+ $下一刻状态 \sim P(⋅|当前状态,智能体的动作)$

## 三、强化学习的目标

1. 智能体的整体回报(return)：整个交互过程的每一轮获得的奖励信号的累加
2. 价值(value)：回报的期望——强化学习中智能体学习的优化目标

## 四、强化学习中的数据

1. **有监督学习**通过优化目标函数来从**固定数据集**中找到模型的最佳参数

   **强化学习**通过智能体与环境的互动来获取数据，不同策略会导致不同的数据分布

2. 占用度量(occupancy measure)：归一化的占用度量衡量智能体决策与动态环境交互过程中状态动作对 (state-action pair)的概率
   + 性质：如果一个智能体的策略有所改变，那么它和环境交互得到的占用度量也会相应改变
3. 强化学习的本质思维
   + 强化学习的难点：智能体看到的数据分布是随着智能体的学习而不断发生改变的
   + 寻找最优策略 = 寻找最优占用度量

## 五、强化学习的独特性

1. 有监督学习任务：找到一个最优的模型函数，使其在训练数据集上最小化一个给定的损失函数。在训练数据独立同分布的假设下，这个优化目标表示最小化模型在整个数据分布上的泛化误差(generalization error)
   $$
   最优模型 = arg\min_{模型} \mathbb{E}_{(特征, 标签) \sim 数据分布}[损失函数(标签, 模型(特征))]
   $$

2. 强化学习任务：最大化智能体策略在和动态环境交互过程中的价值。策略的价值可以等价转换成奖励函数在策略的占用度量/数据分布上的期望
   $$
   最优策略 = \arg\min_{策略} \mathbb{E}_{(状态, 动作) \sim 策略的占用度量} [奖励函数(状态, 动作)]
   $$

3. 有监督学习和强化学习的区别

   |   特性   |                         有监督学习                         |                           强化学习                           |
   | :------: | :--------------------------------------------------------: | :----------------------------------------------------------: |
   | 优化目标 |              优化某个数据分布下的分数值的期望              |               优化某个数据分布下的分数值的期望               |
   | 优化途径 |                 优化目标函数而数据分布不变                 |                  优化数据分布而目标函数不变                  |
   |  关注点  | 寻找一个模型，使其在给定数据分布下得到的损失函数的期望最小 | 寻找一个智能体策略，使其在与动态环境交互的过程中产生最优的数据分布，即最大化该分布下一个给定奖励函数的期望 |

   

